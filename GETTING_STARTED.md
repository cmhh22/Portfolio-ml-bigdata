# ğŸš€ Getting Started

## Prerequisites

- Python 3.9+
- Kaggle account (for dataset download)
- ~500MB disk space

---

## Step 1: Environment Setup

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

## Step 2: Download Dataset

### Option A: Manual Download
1. Go to [Kaggle NYC Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration/data)
2. Download `train.csv` and `test.csv`
3. Place them in `data/raw/`

### Option B: Kaggle API
```bash
# Install and configure Kaggle API
pip install kaggle
# Place kaggle.json in ~/.kaggle/

# Download
kaggle competitions download -c nyc-taxi-trip-duration
unzip nyc-taxi-trip-duration.zip -d data/raw/
```

---

## Step 3: Run Notebooks (in order)

```bash
jupyter notebook notebooks/
```

Execute in this order:

| # | Notebook | Purpose | Output |
|---|----------|---------|--------|
| 1 | `01_data_exploration.ipynb` | EDA, data understanding | Visualizations in `visualizations/` |
| 2 | `02_feature_engineering.ipynb` | Create features | `data/processed/` files |
| 3 | `03_modeling.ipynb` | Train & compare models | `models/` saved models |
| 4 | `04_model_interpretation.ipynb` | SHAP analysis | Interpretation plots |

---

## Step 4: (Optional) Streamlit App

After training, you can run the interactive demo:

```bash
streamlit run app/streamlit_app.py
```

---

## Troubleshooting

### Memory Issues
If you run out of RAM with the full dataset:
```python
# In notebooks, use sampling
from src.data_loader import load_data
df = load_data(sample_size=100000)  # Use 100k sample
```

### Kaggle API Issues
```bash
# Ensure kaggle.json has correct permissions
chmod 600 ~/.kaggle/kaggle.json
```

---

## Project Structure After Running

```
portfolio-ml-bigdata/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/train.csv              # Downloaded from Kaggle
â”‚   â”œâ”€â”€ raw/test.csv               # Downloaded from Kaggle
â”‚   â””â”€â”€ processed/                 # â† Generated by notebook 02
â”‚       â”œâ”€â”€ train_processed.parquet
â”‚       â”œâ”€â”€ val_processed.parquet
â”‚       â””â”€â”€ feature_config.json
â”œâ”€â”€ models/                        # â† Generated by notebook 03
â”‚   â”œâ”€â”€ xgboost_model.pkl          # Best model (RÂ²=0.8234)
â”‚   â”œâ”€â”€ lightgbm_model.pkl
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â”œâ”€â”€ gradient_boosting_model.pkl
â”‚   â”œâ”€â”€ ridge_model.pkl
â”‚   â”œâ”€â”€ feature_importance.csv
â”‚   â””â”€â”€ model_results.csv
â””â”€â”€ visualizations/                # â† Generated by notebooks
```

